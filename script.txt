Hi everyone! I'm excited to talk to you today about spatialsample, and how you can do spatial cross-validation as part of tidymodels workflows.

Before I get into that, I want to introduce myself quickly -- I'm Mike and I'm currently a PhD candidate at SUNY ESF, but the work I'm going to talk about today actually comes from this past summer when I was an intern with the tidymodels team at Posit working on cross validation in tidymodels.

And for anyone who isn't familiar, cross validation is a model assessment approach that helps you understand how well your model will predict new data that it hasn't seen before. Normally, in order to assess a predictive model you'll split all the data you have into a "training" set, which is used to actually fit the model, and an independent "testing" set, which you won't touch until you're done building your models and ready to evaluate them on data they've never seen before. But because you're probably going to want to try a handful of different models, and you want to keep that test data entirely independent from your modeling process, you need to find a way to evaluate intermediate models.

And so what most people do is split their training data into a handful of what we call "folds". You then usually train your model on all but one of those folds, and evaluate it against that final fold, in order to get a sense of how well your intermediate models will do on independent data.

And there's this tidymodels package called rsample that handles that splitting process for you. rsample has a ton of different functions for different cross-validation approaches, which have a pretty standard user interface and help avoid some of the common pitfalls that can happen when using cross-validation. And the objects that rsample functions return can be used with functions from across the tidymodels ecosystem, which makes it easy to use cross-validation as part of any modeling workflow you might want.

But a challenge is that most rsample functions assume you can assign your data to folds at random, and still get independent training and test sets. That's true for a lot of data, but starts to break down once your data is no longer entirely independent -- once you start getting autocorrelation between your observations. And for spatial data, you almost always have autocorrelation. For instance, this is a map of tree cover in Boston, and as you can kinda guess tree cover isn't randomly distributed across the city -- down in Stony Brook Park there's a ton of tree cover, up here in Seaport there's a bit less. The tree cover for any one of these hexagons is pretty tightly linked to how much tree cover its neighbors have.

And so if you split this up at random, you're going to get pretty highly related training and testing sets, which is probably going to make your model assessments way too optimistic. Your training and testing sets are so similar that you're practically testing with training data.

So this is where spatialsample comes in. spatialsample is a newer tidymodels package that helps you do spatial cross-validation, where you assign data to folds based on its spatial location rather than just at random. And it builds on top of rsample, so that the objects from spatialsample work exactly the same way as the ones from rsample and can be used with the rest of tidymodels right out of the box.

So to give you a sense of what that looks like, this is the output from the spatial_clustering_cv() function. You can see that rather than having our folds all mixed in together, we're assigning folds based on where each of these hexagons are located.

And so rather than training and testing with random data, we're testing on data that's geographically separated from our training data, and so hopefully less related to the testing data.

There's a handful of other methods in spatialsample that are also useful; for instance, spatial_block_cv() lets you split your data up using a regular grid, which is a super popular method in ecology studies.

And we've also got a method we call "leave one disc out", or LODO, where you use all the observations within a certain distance of some point as your test set, and leave all the points within a "buffer" distance of that out entirely. And that buffer is really useful to make sure that you aren't including correlated points in both your training and testing data.

And a cool thing about spatialsample is that you can actually use buffers with any of the methods, by passing the buffer argument. So this GIF is using buffers with spatial clustering, for instance; the blue hexagons aren't used in either the training or testing data.

And these methods are all typically going to give you a more accurate model assessment when you're working with spatial data than non-spatial cross-validation. This graph is from a preprint we put out last month -- that green zone is the "target" range for RMSE, and you can see that clustered, LODO, and blocked all have more of their distribution in the green zone than normal V-fold cross-validation. 

And last but not least, I wanted to mention that spatialsample is really designed around the idea of helping users fall into what you'd call a pit of success, and so handles a lot of the common edge cases that come up with spatial data -- spatialsample can work with geographic coordinates, with mismatched coordinate reference systems, with different units, with points, polygons, and lines, and generally does what you'd hope spatial software would do. And by building on top of rsample, the objects and functions from spatialsample automatically integrate with the rest of the tidymodels ecosystenm too.

So, with that, I want to say thanks! If you have any questions or just want to talk, feel free to say hi after; otherwise, you can find me online at MikeMahoney218 or my website, mm218 dot dev. Thanks!
